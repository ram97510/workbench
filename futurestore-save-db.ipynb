{"cells":[{"cell_type":"code","execution_count":10,"id":"aa6c8628-80df-4f58-b957-05cc4ac9e84f","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Collecting psycopg2-binary\n  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nDownloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: psycopg2-binary\nSuccessfully installed psycopg2-binary-2.9.10\n"}],"source":"!pip install psycopg2-binary\n"},{"cell_type":"code","execution_count":null,"id":"6ac5ebec-e332-4203-a052-108905db38e6","metadata":{"trusted":true},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"id":"0b7916d7-2d4d-4121-a5ca-63a0e05f6c44","metadata":{"trusted":true},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"id":"75a91d76-5a90-4642-a975-25886ad494e5","metadata":{"trusted":true},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"id":"d07f7c35-92ed-42d3-9682-56718ffa8c43","metadata":{"trusted":true},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"id":"988792bb-862e-475f-8279-429d333c458f","metadata":{"trusted":true},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"id":"4451f445-1596-45e1-a5c0-c04f0c78d3de","metadata":{"trusted":true},"outputs":[],"source":"df = pd.read_csv(r'insurance_Claims_analysis_main.csv')"},{"cell_type":"code","execution_count":null,"id":"5fdefb17-b9a8-4aac-b659-61075d5581b5","metadata":{"trusted":true},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"id":"640c9f5d-0b05-4fd0-aeec-fc8efbf5b448","metadata":{"trusted":true},"outputs":[],"source":""},{"cell_type":"code","execution_count":2,"id":"5ec1afa5-7f7a-48b5-a082-ede24f114693","metadata":{"trusted":true},"outputs":[],"source":"import pandas as pd"},{"cell_type":"code","execution_count":4,"id":"4dd0332e-482a-483c-b292-25da4b6850d3","metadata":{"trusted":true},"outputs":[],"source":"import pandas as pd\n\ndf.to_json(r'insurance_Claims_analysis_main.json', orient='records', lines=True)\n"},{"cell_type":"code","execution_count":13,"id":"a43d8cf0-d951-45e7-90f2-b40869730988","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"   policy_state_IN  policy_state_OH  insured_hobbies_basketball  \\\n0            False             True                       False   \n1             True            False                       False   \n2            False             True                       False   \n3            False            False                       False   \n4            False            False                       False   \n\n   insured_hobbies_board-games  insured_hobbies_bungie-jumping  \\\n0                        False                           False   \n1                        False                           False   \n2                         True                           False   \n3                         True                           False   \n4                         True                           False   \n\n   insured_hobbies_camping  insured_hobbies_chess  insured_hobbies_cross-fit  \\\n0                    False                  False                      False   \n1                    False                  False                      False   \n2                    False                  False                      False   \n3                    False                  False                      False   \n4                    False                  False                      False   \n\n   insured_hobbies_dancing  insured_hobbies_exercise  ...  vehicle_claim  \\\n0                    False                     False  ...          52080   \n1                    False                     False  ...           3510   \n2                    False                     False  ...          23100   \n3                    False                     False  ...          50720   \n4                    False                     False  ...           4550   \n\n   auto_year  fraud_reported  policy_bind_year  pclaim_severity_int  \\\n0       2004               1              2014                26040   \n1       2007               1              2006                  780   \n2       2007               0              2000                 3850   \n3       2014               1              1990                12680   \n4       2009               0              2014                  650   \n\n   vclaim_severity_int  iclaim_severity_int  tclaim_severity_int  \\\n0               104160                13020               143220   \n1                 3510                  780                 5070   \n2                23100                 7700                34650   \n3               101440                12680               126800   \n4                 4550                 1300                 6500   \n\n   prem_claim_int  umlimit_tclaim_int  \n0     100748825.1                   0  \n1       6069905.4         25350000000  \n2      48965301.0        173250000000  \n3      89757916.0        380400000000  \n4      10295415.0         39000000000  \n\n[5 rows x 110 columns]\n"}],"source":"import pandas as pd\n\n# Load the JSON dataset with lines=True to handle each JSON object on a separate line\ndf = pd.read_json(r'insurance_Claims_analysis_main.json', lines=True)\n\n# Check the dataframe\nprint(df.head())\n"},{"cell_type":"code","execution_count":1,"id":"fd037ca0-ad62-45cf-acf5-882c7fc8ea4b","metadata":{"trusted":true},"outputs":[],"source":"# import psycopg2\n# import json\n\n# # Database connection parameters\n# db_config = {\n#     'dbname': 'postgres',\n#     'user': 'postgres',\n#     'password': 'password',\n#     'host': 'host.docker.internal',  # Ensure the host is correct\n#     'port': 5432          # Custom port is 5433 (from your Docker mapping)\n# }\n\n# # Path to your JSON file\n# json_file_path = 'insurance_Claims_analysis_main.json'  # Use the file path directly\n\n# # Read JSON data from the file (handling multiple lines)\n# json_data = []\n# with open(json_file_path, 'r') as file:\n#     for line in file:\n#         try:\n#             json_data.append(json.loads(line))  # Parse each line as a JSON object\n#         except json.JSONDecodeError as e:\n#             print(f\"Skipping invalid JSON line: {e}\")\n\n# # Initialize connection variable\n# conn = None  \n\n# # Connect to the PostgreSQL database\n# try:\n#     conn = psycopg2.connect(**db_config)\n#     cursor = conn.cursor()\n\n#     # Insert each JSON object into the `future` table\n#     insert_query = \"INSERT INTO future (data) VALUES (%s);\"\n#     for record in json_data:\n#         cursor.execute(insert_query, (json.dumps(record),))  # Convert to string for insertion\n\n#     # Commit the transaction\n#     conn.commit()\n#     print(\"Data inserted successfully into the 'future' table!\")\n\n# except Exception as e:\n#     print(f\"An error occurred: {e}\")\n\n# finally:\n#     # Close the database connection safely\n#     if conn is not None:\n#         cursor.close()\n#         conn.close()\n"},{"cell_type":"code","execution_count":43,"id":"648ae7ac-9f7b-4eee-8ac7-4ec9a9a6fb6e","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Data inserted successfully into the 'new_table'!\n"}],"source":"import psycopg2\nimport json\n\n# Database connection parameters\ndb_config = {\n    'dbname': 'postgres',\n    'user': 'postgres',\n    'password': 'password',\n    'host': '127.0.0.1',  # Ensure the host is correct\n    'port': 5433  # Custom port is 5433 (from your Docker mapping)\n}\n\n# Path to your JSON file\njson_file_path = 'insurance_Claims_analysis_main.json'  # Use the file path directly\n\n# Read JSON data from the file (handling multiple lines)\njson_data = []\nwith open(json_file_path, 'r') as file:\n    for line in file:\n        try:\n            json_data.append(json.loads(line))  # Parse each line as a JSON object\n        except json.JSONDecodeError as e:\n            print(f\"Skipping invalid JSON line: {e}\")\n\n# Initialize connection variable\nconn = None  \n\n# Connect to the PostgreSQL database\ntry:\n    conn = psycopg2.connect(**db_config)\n    cursor = conn.cursor()\n\n    # Create a new table if it doesn't exist\n    create_table_query = '''\n    CREATE TABLE IF NOT EXISTS futures (\n        id SERIAL PRIMARY KEY,\n        data JSONB\n    );\n    '''\n    cursor.execute(create_table_query)\n\n    # Insert each JSON object into the `new_table`\n    insert_query = \"INSERT INTO futures (data) VALUES (%s);\"\n    for record in json_data:\n        cursor.execute(insert_query, (json.dumps(record),))  # Convert to string for insertion\n\n    # Commit the transaction\n    conn.commit()\n    print(\"Data inserted successfully into the 'new_table'!\")\n\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n\nfinally:\n    # Close the database connection safely\n    if conn is not None:\n        cursor.close()\n        conn.close()\n"},{"cell_type":"code","execution_count":45,"id":"64960f25-c3b9-486d-a056-d5ee145b98b3","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"   id                                               data\n0   1  {'age': 48, 'auto_year': 2004, 'witnesses': 2,...\n1   2  {'age': 42, 'auto_year': 2007, 'witnesses': 0,...\n2   3  {'age': 29, 'auto_year': 2007, 'witnesses': 3,...\n3   4  {'age': 41, 'auto_year': 2014, 'witnesses': 2,...\n4   5  {'age': 44, 'auto_year': 2009, 'witnesses': 1,...\n"},{"name":"stderr","output_type":"stream","text":"/tmp/ipykernel_19050/4163880353.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n  df = pd.read_sql(query, conn)\n"}],"source":"import psycopg2\nimport pandas as pd\n\n# Database configuration\ndb_config = {\n    'dbname': 'postgres',\n    'user': 'postgres',\n    'password': 'password',\n    'host': 'host.docker.internal',  # Ensure the host is correct\n    'port': 5432          # Custom port is 5433 (from your Docker mapping)\n}\n\n\n# Connect to PostgreSQL and fetch data\ntry:\n    conn = psycopg2.connect(**db_config)\n    query = \"SELECT * FROM futures;\"\n    df = pd.read_sql(query, conn)\n    conn.close()\n    \n    print(df.head())  # Display first few rows\nexcept Exception as e:\n    print(\"Error:\", e)\n"},{"cell_type":"code","execution_count":null,"id":"dc796de5-fd39-4dcd-9621-4a64c9de1a07","metadata":{"trusted":true},"outputs":[],"source":""},{"cell_type":"code","execution_count":46,"id":"719e4213-c002-4fa8-b41b-f418e9e10585","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"An error occurred: connection to server at \"127.0.0.1\", port 5433 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?\n\n"}],"source":"import psycopg2\nimport json\n\n# Database connection parameters\ndb_config = {\n    'dbname': 'postgres',\n    'user': 'postgres',\n    'password': 'password',\n    'host': '127.0.0.1',  # Ensure the host is correct\n    'port': 5433  # Custom port is 5433 (from your Docker mapping)\n}\n\n# Path to your JSON file\njson_file_path = 'insurance_Claims_analysis_main.json'  # Use the file path directly\n\n# Read JSON data from the file (handling multiple lines)\njson_data = []\nwith open(json_file_path, 'r') as file:\n    for line in file:\n        try:\n            json_data.append(json.loads(line))  # Parse each line as a JSON object\n        except json.JSONDecodeError as e:\n            print(f\"Skipping invalid JSON line: {e}\")\n\n# Initialize connection variable\nconn = None  \n\n# Connect to the PostgreSQL database\ntry:\n    conn = psycopg2.connect(**db_config)\n    cursor = conn.cursor()\n\n    # Create a new table if it doesn't exist\n    create_table_query = '''\n    CREATE TABLE IF NOT EXISTS futures (\n        id SERIAL PRIMARY KEY,\n        data JSONB\n    );\n    '''\n    cursor.execute(create_table_query)\n\n    # Insert each JSON object into the `new_table`\n    insert_query = \"INSERT INTO futures (data) VALUES (%s);\"\n    for record in json_data:\n        cursor.execute(insert_query, (json.dumps(record),))  # Convert to string for insertion\n\n    # Commit the transaction\n    conn.commit()\n    print(\"Data inserted successfully into the 'new_table'!\")\n\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n\nfinally:\n    # Close the database connection safely\n    if conn is not None:\n        cursor.close()\n        conn.close()\n"},{"cell_type":"code","execution_count":48,"id":"2b7b55aa-2e9d-4336-96ba-67f4ce37c5b4","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Error: connection to server at \"127.0.0.1\", port 5433 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?\n\n"}],"source":"import psycopg2\nimport pandas as pd\n\n# Database configuration\ndb_config = {\n    'dbname': 'postgres',\n    'user': 'postgres',\n    'password': 'password',\n    'host': '127.0.0.1',  # Ensure the host is correct\n    'port': 5433          # Custom port is 5433 (from your Docker mapping)\n}\n\n\n# Connect to PostgreSQL and fetch data\ntry:\n    conn = psycopg2.connect(**db_config)\n    query = \"SELECT * FROM futures;\"\n    df = pd.read_sql(query, conn)\n    conn.close()\n    \n    print(df.head())  # Display first few rows\nexcept Exception as e:\n    print(\"Error:\", e)"},{"cell_type":"code","execution_count":null,"id":"48df0c68-3ff5-48d8-ae8e-e4fbf8ef94e1","metadata":{"trusted":true},"outputs":[],"source":"DATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql',\n        'NAME': 'postgres',  # Your actual database name\n        'USER': 'postgres',\n        'PASSWORD': 'password',\n        'HOST': '127.0.0.1',  # Change this to 127.0.0.1\n        'PORT': '5433',  # Ensure PostgreSQL is listening on this port\n    }\n}"},{"cell_type":"code","execution_count":null,"id":"013539c1-711b-4322-ba52-c0d74588e964","metadata":{"trusted":true},"outputs":[],"source":"import psycopg2\nimport pandas as pd\nfrom dotenv import load_dotenv\nimport os\nfrom feast import Entity, FeatureStore, FeatureView, Field, FileSource, FeatureService\nfrom feast.types import Bool, Int64, String, Float64\nfrom feast.infra.offline_stores.file_source import SavedDatasetFileStorage\nfrom datetime import datetime, timedelta\nimport json\n\n# Load environment variables from the .env file\nload_dotenv()\n\n# Database configuration from the .env file\ndb_config = {\n    'dbname': os.getenv('DB_NAME'),\n    'user': os.getenv('DB_USER'),\n    'password': os.getenv('DB_PASSWORD'),\n    'host': os.getenv('DB_HOST'),\n    'port': os.getenv('DB_PORT')\n}\n\n# Fetch data from PostgreSQL\ntry:\n    conn = psycopg2.connect(**db_config)\n    query = \"SELECT * FROM futures;\"  # Replace with your actual query\n    df_postgres = pd.read_sql(query, conn)\n    conn.close()\n    \n    print(\"PostgreSQL Data: \", df_postgres.head())  # Display first few rows of PostgreSQL data\nexcept Exception as e:\n    print(\"Error:\", e)\n\n\n# Use the fetched PostgreSQL data for Feast pipeline\njson_file_path = os.getenv('df')\nparquet_file_path = os.getenv('PARQUET_FILE_PATH')\ndataset_file_path = os.getenv('DATASET_FILE_PATH')\n\n# Assuming the JSON is loaded from a file for reference\nwith open(json_file_path, 'r') as f:\n    json_data = json.load(f)\n\n# Create DataFrame from JSON data (or you can use df_postgres if that's what you're using)\nsample_data = json_data\nentity_df = pd.DataFrame(sample_data)\n\n# Set timestamp columns\nentity_df['event_timestamp'] = pd.to_datetime(\"2024-01-01\")\nentity_df['created_timestamp'] = datetime.now()\n\n# Define data types for Feast fields\ndtype_mapping = {\n    'int64': Int64,\n    'float64': Float64,\n    'float32': Float64,\n    'bool': Bool,\n    'object': String  \n}\n\n# Create fields dynamically\nfields = []\nfor column_name in entity_df.columns:\n    if column_name not in ['event_timestamp', 'created_timestamp']:\n        pandas_dtype = entity_df[column_name].dtype.name  \n        feast_dtype = dtype_mapping.get(pandas_dtype, String)  \n        fields.append(Field(name=column_name, dtype=feast_dtype))\n\njson_file_name = os.path.splitext(os.path.basename(json_file_path))[0]\n\n# Define the Entity for Feast (for PostgreSQL data or any other entity you're working with)\ndob_ssn = Entity(\n    name=\"dob_ssn\",\n    description=\"Date of birth and last four digits of social security number\",\n    tags={\"owner\": \"in10s\", \"team\": \"innovation\"},\n)\n\n# Save DataFrame as Parquet (for Feast FileSource)\nentity_df.to_parquet(parquet_file_path, index=False)\n\n# Create FileSource for Feast\ndata_source = FileSource(\n    name=json_file_name,\n    path=parquet_file_path,  \n    timestamp_field=\"event_timestamp\",\n)\n\n# Define FeatureView in Feast\ndata_feature_view = FeatureView(\n    name=json_file_name,\n    ttl=timedelta(days=9000),\n    schema=fields,\n    source=data_source,\n    tags={\"date_added\": \"2024-11-24\", \"experiments\": \"experiment-A\", \"access_group\": \"in10s\"},\n    online=True,\n)\n\n# Create FeatureService (this is a high-level abstraction)\nfirst_three_columns = list(entity_df.columns)[:3]\ndata_feature_service = FeatureService(\n    name=json_file_name,\n    features=[data_feature_view[first_three_columns]],\n    tags={\"owner\": \"in10s@intense.in\", \"stage\": \"dev\"},\n    description=\"dataset service\",\n)\n\n# Initialize FeatureStore\nstore = FeatureStore(repo_path='.')\n\n# Register the Feature View in the Feature Store\nstore.apply([data_feature_view, dob_ssn])\n\n# Generate feature names dynamically\nfeature_names = [f\"{json_file_name}:{column_name}\" for column_name in entity_df.columns if column_name not in ['event_timestamp', 'created_timestamp']]\n\n# If the dataset file exists, remove it\nif os.path.exists(dataset_file_path):\n    os.remove(dataset_file_path)\n\nprint(\"feature_names:########\", feature_names)\n\ntry:\n    # Retrieve historical features\n    training_data = store.get_historical_features(\n        entity_df=entity_df,\n        features=feature_names\n    )\n\n    # Save the dataset\n    dataset = store.create_saved_dataset(\n        from_=training_data,\n        name=\"dataset\",\n        storage=SavedDatasetFileStorage(dataset_file_path)\n    )\n\n    print(f\"Dataset saved successfully to {dataset_file_path}.\")\nexcept Exception as e:\n    print(f\"Error retrieving historical features: {e}\")\n"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":5}